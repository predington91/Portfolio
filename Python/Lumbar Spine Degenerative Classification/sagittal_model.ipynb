{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":9020984,"sourceType":"datasetVersion","datasetId":5430713}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#imports needed packages\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pathlib import Path\nimport pydicom\n\nimport os\nfrom os.path import join\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import convnext_small\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:22:41.011473Z","iopub.execute_input":"2024-07-27T00:22:41.012260Z","iopub.status.idle":"2024-07-27T00:22:49.004629Z","shell.execute_reply.started":"2024-07-27T00:22:41.012221Z","shell.execute_reply":"2024-07-27T00:22:49.003784Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Creates config class\nclass CFG:\n    verbose = 1\n    seed = 21\n    sag_labels = 45\n    axial_labels = 30\n    image_size = [512,512]\n    epochs = 6\n    batch_size = 12\n    patience = 2\n    learning_rate = 0.001\n    \n# Set seed for reproducibility\ntorch.manual_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:22:49.006737Z","iopub.execute_input":"2024-07-27T00:22:49.007412Z","iopub.status.idle":"2024-07-27T00:22:49.020052Z","shell.execute_reply.started":"2024-07-27T00:22:49.007372Z","shell.execute_reply":"2024-07-27T00:22:49.019066Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7cf72d696330>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dataset And Transformations","metadata":{}},{"cell_type":"code","source":"class MRIDataset(Dataset):\n    def __init__(self, dataframe, base_dir, transform=None):\n        self.dataframe = dataframe\n        self.base_dir = base_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_filenames = [self.dataframe.iloc[idx, i] for i in range(1, 25)]\n        img_paths = [os.path.join(self.base_dir, fname) for fname in img_filenames]\n        images = []\n\n        for img_path in img_paths:\n            dicom_data = pydicom.dcmread(img_path)\n            image = dicom_data.pixel_array.astype(float)\n\n            if dicom_data.PhotometricInterpretation == \"MONOCHROME1\":\n                image = np.amax(image) - image\n\n            image = (image - np.mean(image)) / np.std(image)\n            image = Image.fromarray((image * 255).astype(np.uint8))\n\n            if self.transform:\n                image = self.transform(image)\n\n            images.append(image)\n\n        # Concatenate images along the channel dimension\n        images = torch.cat(images, dim=0)\n        \n        # Check and remove the unnecessary singleton dimension\n        if images.dim() == 5:\n            images = images.squeeze(2)\n\n        labels = self.dataframe.iloc[idx, 25:].values.astype(float)\n        return images, labels","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:22:49.021380Z","iopub.execute_input":"2024-07-27T00:22:49.021738Z","iopub.status.idle":"2024-07-27T00:22:49.032389Z","shell.execute_reply.started":"2024-07-27T00:22:49.021694Z","shell.execute_reply":"2024-07-27T00:22:49.031365Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Function to create datasets and dataloaders\ndef create_datasets(train_df, val_df, base_dir):\n    transform = transforms.Compose([\n        transforms.Resize((CFG.image_size[0], CFG.image_size[1])),\n        transforms.RandomRotation(degrees=15),\n        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15)),\n        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5], std=[0.2])\n    ])\n\n    train_dataset = MRIDataset(train_df, base_dir=base_dir, transform=transform)\n    val_dataset = MRIDataset(val_df, base_dir=base_dir, transform=transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\n\n    return train_loader, val_loader","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:23:01.981697Z","iopub.execute_input":"2024-07-27T00:23:01.982056Z","iopub.status.idle":"2024-07-27T00:23:01.990493Z","shell.execute_reply.started":"2024-07-27T00:23:01.982026Z","shell.execute_reply":"2024-07-27T00:23:01.989434Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model Prep","metadata":{}},{"cell_type":"code","source":"def prepare_model(num_labels):\n    model = convnext_small(pretrained=True)\n    in_channels = 24  #numbe rof images in each input\n    \n    # Modifies the first convolutional layer\n    model.features[0][0] = nn.Conv2d(in_channels, 96, kernel_size=4, stride=4)\n    \n    # Modifies the classifier to match the number of labels\n    model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_labels)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    return model, device","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:23:08.267248Z","iopub.execute_input":"2024-07-27T00:23:08.267611Z","iopub.status.idle":"2024-07-27T00:23:08.274676Z","shell.execute_reply.started":"2024-07-27T00:23:08.267581Z","shell.execute_reply":"2024-07-27T00:23:08.273338Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# function calculates the positive class weights for a set of labels, to handle class imbalances\ndef calculate_pos_weight(labels):\n    pos_weights = []\n    labels = labels.cpu().numpy()\n    num_samples, num_labels = labels.shape\n    for i in range(num_labels):\n        pos_count = labels[:, i].sum()\n        if pos_count == 0:\n            pos_weight = 0.0 \n        else:\n            pos_weight = (num_samples - pos_count) / pos_count\n        pos_weights.append(pos_weight)\n    pos_weight_tensor = torch.tensor(pos_weights, dtype=torch.float32)\n    return pos_weight_tensor","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:23:08.276355Z","iopub.execute_input":"2024-07-27T00:23:08.276696Z","iopub.status.idle":"2024-07-27T00:23:08.286666Z","shell.execute_reply.started":"2024-07-27T00:23:08.276648Z","shell.execute_reply":"2024-07-27T00:23:08.285717Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, device, CFG):\n    # Computes initial positive weights\n    for images, labels in train_loader:\n        pos_weight = calculate_pos_weight(labels)\n        break\n    \n    pos_weight = pos_weight.to(device)\n    \n    # Defines the loss function with class weights\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    \n    # Defines the optimizer\n    optimizer = optim.Adam(model.parameters(), lr=CFG.learning_rate)\n    \n    best_val_loss = float('inf') \n    patience_counter = 0\n\n    # Training loop\n    for epoch in range(CFG.epochs):\n        model.train() \n        running_loss = 0.0\n\n        # Creates a progress bar for the training loop\n        train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{CFG.epochs}', leave=False)\n\n        # Iterates over batches in the training set\n        for images, labels in train_loader_tqdm:\n            images = images.to(device) \n            labels = labels.to(device) \n\n            optimizer.zero_grad()  # Zerosthe parameter gradients\n            outputs = model(images) \n            loss = criterion(outputs, labels)  # Computes loss\n\n            # Checks for NaN values in the loss\n            if torch.isnan(loss):\n                print(\"NaN loss detected, breaking out of training loop.\")\n                return model\n\n            loss.backward() \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clips gradients to avoid exploding gradients\n            optimizer.step()  # Updates model parameters\n            running_loss += loss.item()\n\n            # Update progress bar with the average loss\n            train_loader_tqdm.set_postfix(loss=running_loss/len(train_loader))\n\n        # Compute average training loss for this epoch\n        avg_train_loss = running_loss / len(train_loader)\n\n        \n        \n       # Validation loop\n        val_loss = 0.0  # Initialize validation loss\n\n        model.eval()\n        val_loader_tqdm = tqdm(val_loader, desc='Validation', leave=False)\n\n        with torch.no_grad():  # Disable gradient calculations\n            for images, labels in val_loader_tqdm:\n                images = images.to(device) \n                labels = labels.to(device) \n                outputs = model(images) \n                loss = criterion(outputs, labels) \n\n                if torch.isnan(loss):\n                    print(\"NaN loss detected during validation, breaking out of validation loop.\")\n                    return model\n\n                val_loss += loss.item()\n                val_loader_tqdm.set_postfix(val_loss=val_loss/len(val_loader))\n\n        avg_val_loss = val_loss / len(val_loader)\n\n        # Print epoch results\n        print(f'Epoch [{epoch+1}/{CFG.epochs}], '\n              f'Loss: {avg_train_loss:.4f}, '\n              f'Val Loss: {avg_val_loss:.4f}')\n\n        # Check for early stopping\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss \n            patience_counter = 0 \n        else:\n            patience_counter += 1 \n            if patience_counter >= CFG.patience:\n                print(\"Early stopping\")\n                break  # Stop training if patience is reached\n\n    return model, best_val_loss","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:23:08.288098Z","iopub.execute_input":"2024-07-27T00:23:08.288410Z","iopub.status.idle":"2024-07-27T00:23:08.301864Z","shell.execute_reply.started":"2024-07-27T00:23:08.288387Z","shell.execute_reply":"2024-07-27T00:23:08.300992Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Cross-validation function\ndef cross_validate_model(dataframe, base_dir, num_labels, k_folds=5):\n    kf = KFold(n_splits=k_folds)\n    fold_results = []  \n    best_model = None  \n    best_fold_loss = float('inf') \n\n    # Iterate through each fold\n    for fold, (train_idx, val_idx) in enumerate(kf.split(dataframe)):\n        print(f'Fold {fold+1}/{k_folds}')\n        \n        # Splits dataframe into training and validation sets\n        train_df = dataframe.iloc[train_idx]\n        val_df = dataframe.iloc[val_idx]\n\n        # Creates data loaders for the current fold\n        train_loader, val_loader = create_datasets(train_df, val_df, base_dir)\n        \n        # Prepares model and device\n        model, device = prepare_model(num_labels)\n        \n        # Trains the model and evaluate it on the validation set\n        trained_model, val_loss = train_model(model, train_loader, val_loader, device, CFG)\n\n        # Check if the current fold has the best validation loss\n        if val_loss < best_fold_loss:\n            best_fold_loss = val_loss  \n            best_model = trained_model\n\n        # Store validation loss for the current fold\n        fold_results.append(val_loss)\n        print(f'Fold {fold+1}/{k_folds} - Validation Loss: {val_loss:.4f}')\n\n    # Calculate mean validation loss across all folds\n    mean_val_loss = np.mean(fold_results)\n    print(f'Mean Validation Loss: {mean_val_loss:.4f}')\n    \n    return best_model, mean_val_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:23:08.304419Z","iopub.execute_input":"2024-07-27T00:23:08.304995Z","iopub.status.idle":"2024-07-27T00:23:08.314681Z","shell.execute_reply.started":"2024-07-27T00:23:08.304954Z","shell.execute_reply":"2024-07-27T00:23:08.313710Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Function to save the model\ndef save_model(model, file_path):\n    torch.save(model.state_dict(), file_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:23:08.315824Z","iopub.execute_input":"2024-07-27T00:23:08.316320Z","iopub.status.idle":"2024-07-27T00:23:08.325555Z","shell.execute_reply.started":"2024-07-27T00:23:08.316289Z","shell.execute_reply":"2024-07-27T00:23:08.324750Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"#Imports CSV files needed for notebook\nbase_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\ndf_path = '/kaggle/input/mri-datasets'\n\nsag_df = pd.read_csv(df_path + '/sagittal__df.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:23:08.326707Z","iopub.execute_input":"2024-07-27T00:23:08.327130Z","iopub.status.idle":"2024-07-27T00:23:08.411102Z","shell.execute_reply.started":"2024-07-27T00:23:08.327100Z","shell.execute_reply":"2024-07-27T00:23:08.410336Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"best_model, mean_val_loss = cross_validate_model(sag_df, base_dir, CFG.sag_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T00:23:08.412326Z","iopub.execute_input":"2024-07-27T00:23:08.412622Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/convnext_small-0c510722.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small-0c510722.pth\n100%|██████████| 192M/192M [00:01<00:00, 160MB/s]  \n                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/6], Loss: 0.5081, Val Loss: 0.5113\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/6], Loss: 0.4919, Val Loss: 0.4969\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/6], Loss: 0.4881, Val Loss: 0.4985\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/6], Loss: 0.4858, Val Loss: 0.4936\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/6], Loss: 0.4852, Val Loss: 0.4931\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/6:  77%|███████▋  | 101/132 [18:27<05:32, 10.74s/it, loss=0.369]","output_type":"stream"}]},{"cell_type":"code","source":"save_model(best_model, 'sag_model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}