{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":8962278,"sourceType":"datasetVersion","datasetId":5394464},{"sourceId":9028417,"sourceType":"datasetVersion","datasetId":5441430}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#imports needed packages\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport pickle\nimport pydicom\nfrom pathlib import Path\nfrom os.path import join\nfrom PIL import Image\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models import convnext_small\nimport torch.nn as nn\nimport timm\nfrom timm import create_model\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-27T19:40:49.725002Z","iopub.execute_input":"2024-07-27T19:40:49.725493Z","iopub.status.idle":"2024-07-27T19:40:49.734226Z","shell.execute_reply.started":"2024-07-27T19:40:49.725438Z","shell.execute_reply":"2024-07-27T19:40:49.732735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creates config class\nclass CFG:\n    verbose = 1\n    seed = 21\n    sag_labels = 15\n    sag_channels = 24\n    num_classes = 3\n    axial_channels = 15\n    axial_labels = 10\n    image_size = [512,512]\n    batch_size = 12\n    \n# Set seed for reproducibility\ntorch.manual_seed(CFG.seed)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:49.736752Z","iopub.execute_input":"2024-07-27T19:40:49.737242Z","iopub.status.idle":"2024-07-27T19:40:49.746550Z","shell.execute_reply.started":"2024-07-27T19:40:49.737196Z","shell.execute_reply":"2024-07-27T19:40:49.745290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imports CSV files needed for notebook\ndf_path = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/\"\ntest_df = pd.read_csv(df_path + 'test_series_descriptions.csv')\nscaler_path = \"/kaggle/input/submission-models/y_coordinate_scaler.pkl\"\ny_model_path = \"/kaggle/input/submission-models/y_coordinate_model.pkl\"\nsag_model_path = \"/kaggle/input/submission-models/sag_model.pth\"\naxial_model_path = \"/kaggle/input/submission-models/axial_model.pth\"","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:49.748322Z","iopub.execute_input":"2024-07-27T19:40:49.748867Z","iopub.status.idle":"2024-07-27T19:40:49.760258Z","shell.execute_reply.started":"2024-07-27T19:40:49.748799Z","shell.execute_reply":"2024-07-27T19:40:49.758894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the conditions for each model type\nsag_conditions = ['spinal_canal_stenosis''left_neural_foraminal_narrowing', 'right_neural_foraminal_narrowing']\naxial_conditions = ['left_subarticular_stenosis', 'right_subarticular_stenosis']\n\n# Define the levels\nlevels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:49.761792Z","iopub.execute_input":"2024-07-27T19:40:49.762150Z","iopub.status.idle":"2024-07-27T19:40:49.768514Z","shell.execute_reply.started":"2024-07-27T19:40:49.762121Z","shell.execute_reply":"2024-07-27T19:40:49.767071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Prep","metadata":{}},{"cell_type":"code","source":"class convnext_small(nn.Module):\n    def __init__(self, num_classes, pretrained=False):\n        super(convnext_small, self).__init__()\n        self.model = timm.create_model('convnext_small', pretrained=pretrained, num_classes=num_classes)\n    \n    def forward(self, x):\n        return self.model(x)\n\ndef load_model(in_channels, num_labels, file_path, device):\n    model = convnext_small(pretrained=False, num_classes=num_labels)\n    \n    # Modify the first convolutional layer (stem layer)\n    model.model.stem[0] = nn.Conv2d(in_channels, 96, kernel_size=4, stride=4, padding=1)\n    \n    # Load the model state dict\n    state_dict = torch.load(file_path, map_location=device)\n    \n    # Filter out the keys that are in the state dict and the ones that don't match\n    new_state_dict = {k: v for k, v in state_dict.items() if k in model.state_dict()}\n    model.load_state_dict(new_state_dict, strict=False)\n    \n    model.to(device)\n    model.eval()\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:49.771399Z","iopub.execute_input":"2024-07-27T19:40:49.771797Z","iopub.status.idle":"2024-07-27T19:40:49.786827Z","shell.execute_reply.started":"2024-07-27T19:40:49.771763Z","shell.execute_reply":"2024-07-27T19:40:49.785554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loads Sagittal and Axial models\nsag_model = load_model(CFG.sag_channels, CFG.sag_labels * CFG.num_classes, sag_model_path, device)\naxial_model = load_model(CFG.axial_channels, CFG.axial_labels* CFG.num_classes, axial_model_path, device)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:49.788533Z","iopub.execute_input":"2024-07-27T19:40:49.788994Z","iopub.status.idle":"2024-07-27T19:40:54.121230Z","shell.execute_reply.started":"2024-07-27T19:40:49.788956Z","shell.execute_reply":"2024-07-27T19:40:54.119715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loads the scaler file\nwith open(scaler_path, 'rb') as f:\n    scaler = pickle.load(f)\n#loads the Y coordinate model\nwith open(y_model_path, 'rb') as f:\n    y_coor_model = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.123229Z","iopub.execute_input":"2024-07-27T19:40:54.123780Z","iopub.status.idle":"2024-07-27T19:40:54.161914Z","shell.execute_reply.started":"2024-07-27T19:40:54.123732Z","shell.execute_reply":"2024-07-27T19:40:54.160024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sagttal Data Prep","metadata":{}},{"cell_type":"code","source":"#creates function to pull file list from series fodlers\ndef get_file_list(study_id, series_id):\n    data_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n    files = os.listdir(os.path.join(data_dir, str(study_id), str(series_id)))\n    file_paths = [os.path.join(data_dir, str(study_id), str(series_id), file) for file in files if file.endswith('.dcm')]\n    return file_paths","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.163931Z","iopub.execute_input":"2024-07-27T19:40:54.164371Z","iopub.status.idle":"2024-07-27T19:40:54.172666Z","shell.execute_reply.started":"2024-07-27T19:40:54.164334Z","shell.execute_reply":"2024-07-27T19:40:54.170900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expand_dataframe(df, max_files=12):\n    # Prepare a list to store the new columns\n    new_columns = {f'file_path_{i+1}': [] for i in range(max_files)}\n\n    # Iterate through each row in the DataFrame\n    for index, row in df.iterrows():\n        study_id = row['study_id']\n        series_id = row['series_id']\n\n        # Get the list of files for studt/series id\n        file_list = get_file_list(study_id, series_id)\n\n        # checks for enough files, and duplicates file paths if not\n        if len(file_list) < max_files:\n            file_list = (file_list * ((max_files // len(file_list)) + 1))[:max_files]\n        else:\n            file_list = file_list[:max_files]\n\n        # Fill the new columns with file paths\n        for i in range(max_files):\n            new_columns[f'file_path_{i+1}'].append(file_list[i] if i < len(file_list) else None)\n\n    new_columns_df = pd.DataFrame(new_columns)\n    expanded_df = pd.concat([df.reset_index(drop=True), new_columns_df], axis=1)\n\n    return expanded_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.174934Z","iopub.execute_input":"2024-07-27T19:40:54.175420Z","iopub.status.idle":"2024-07-27T19:40:54.194360Z","shell.execute_reply.started":"2024-07-27T19:40:54.175323Z","shell.execute_reply":"2024-07-27T19:40:54.192948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_file_paths(df, max_files=12):\n    combined_data = {}\n\n    # Iterate through each row in the DataFrame\n    for _, row in df.iterrows():\n        study_id = row['study_id']\n        \n        # Extract file paths from the row\n        file_paths = [row[f'file_path_{i+1}'] for i in range(max_files) if pd.notna(row[f'file_path_{i+1}'])]\n\n        # Creates study id rows if needed\n        if study_id not in combined_data:\n            combined_data[study_id] = [None] * (max_files * 2) \n\n        # Finds the next available index for the file paths\n        existing_paths = [path for path in combined_data[study_id] if path is not None]\n        start_index = len(existing_paths)\n\n        # Adds file paths to the combined data\n        for i, path in enumerate(file_paths):\n            if start_index + i < len(combined_data[study_id]):\n                combined_data[study_id][start_index + i] = path\n\n    # Create a DataFrame from the combined data\n    combined_df = pd.DataFrame.from_dict(combined_data, orient='index', columns=[f'file_path_{i+1}' for i in range(max_files * 2)])\n\n    # Reset index to have study id as a column\n    combined_df.reset_index(inplace=True)\n    combined_df.rename(columns={'index': 'study_id'}, inplace=True)\n\n    return combined_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.195910Z","iopub.execute_input":"2024-07-27T19:40:54.196566Z","iopub.status.idle":"2024-07-27T19:40:54.211538Z","shell.execute_reply.started":"2024-07-27T19:40:54.196520Z","shell.execute_reply":"2024-07-27T19:40:54.209790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to reuse file paths to fill empty columns\ndef fill_empty_paths(df):\n    for index, row in df.iterrows():\n        replace_image = row['file_path_1']  \n        count = 0\n        for i in range(1, 25):\n            col_name = f'file_path_{i}'\n            if pd.isna(row[col_name]):\n                count += 1\n                replace_image = row[f'file_path_{count}']\n                df.at[index, col_name] = replace_image\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.213898Z","iopub.execute_input":"2024-07-27T19:40:54.214663Z","iopub.status.idle":"2024-07-27T19:40:54.232862Z","shell.execute_reply.started":"2024-07-27T19:40:54.214561Z","shell.execute_reply":"2024-07-27T19:40:54.230576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sag_intial_df = test_df[(test_df['series_description'] == 'Sagittal T1') | (test_df['series_description'] == 'Sagittal T2/STIR')]\ndrop_df = sag_intial_df.drop_duplicates(subset=['study_id', 'series_id'])\nexpanded_df = expand_dataframe(drop_df)\ncombine_df = combine_file_paths(expanded_df)\nsagittal_df = fill_empty_paths(combine_df)\nsagittal_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.234749Z","iopub.execute_input":"2024-07-27T19:40:54.235685Z","iopub.status.idle":"2024-07-27T19:40:54.277842Z","shell.execute_reply.started":"2024-07-27T19:40:54.235633Z","shell.execute_reply":"2024-07-27T19:40:54.276388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get Y Coordinates","metadata":{}},{"cell_type":"code","source":"#creates a row for each level of a series id\ndef get_levels(df, levels):\n\n    rows = []\n    for idx, row in df.iterrows():\n\n        for level in levels:\n            new_row = row.copy()\n            new_row['level'] = level\n            rows.append(new_row)\n    \n    result_df = pd.DataFrame(rows)\n    \n    return result_df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.279551Z","iopub.execute_input":"2024-07-27T19:40:54.279951Z","iopub.status.idle":"2024-07-27T19:40:54.287737Z","shell.execute_reply.started":"2024-07-27T19:40:54.279916Z","shell.execute_reply":"2024-07-27T19:40:54.286328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creates function to pull file list from series fodlers\ndef get_first_file_path(study_id, series_id):\n    data_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n    series_path = os.path.join(data_dir, str(study_id), str(series_id))\n    first_file = next(file for file in os.listdir(series_path) if file.endswith('.dcm'))\n    file_path_1 = os.path.join(series_path, first_file)\n    \n    return file_path_1","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.294235Z","iopub.execute_input":"2024-07-27T19:40:54.294894Z","iopub.status.idle":"2024-07-27T19:40:54.302974Z","shell.execute_reply.started":"2024-07-27T19:40:54.294830Z","shell.execute_reply":"2024-07-27T19:40:54.301476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to get image metadata\ndef get_shape(image_path):\n    dicom = pydicom.dcmread(image_path)\n    height = dicom.Rows\n    width = dicom.Columns\n    x_pixel_spacing, y_pixel_spacing  = dicom.PixelSpacing\n    x_image_position,y_image_position,z_image_position = dicom.ImagePositionPatient\n    \n    return height, width, x_image_position,y_image_position,z_image_position,x_pixel_spacing,y_pixel_spacing","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.304714Z","iopub.execute_input":"2024-07-27T19:40:54.305125Z","iopub.status.idle":"2024-07-27T19:40:54.321193Z","shell.execute_reply.started":"2024-07-27T19:40:54.305090Z","shell.execute_reply":"2024-07-27T19:40:54.319524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coor_intial_df = test_df[(test_df['series_description'] == 'Sagittal T1')]\nsag_level_df = get_levels(coor_intial_df, levels)\ncoor_level_df=sag_level_df.copy()\n#runs file path function and creates a columns for them\ncoor_level_df['file_path_1'] = sag_level_df.apply(lambda row: get_first_file_path(row['study_id'], row['series_id']), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.340874Z","iopub.execute_input":"2024-07-27T19:40:54.342193Z","iopub.status.idle":"2024-07-27T19:40:54.368039Z","shell.execute_reply.started":"2024-07-27T19:40:54.342148Z","shell.execute_reply":"2024-07-27T19:40:54.366530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#runs metadata functions and creates columsn for new variables\nfor index, row in coor_level_df.iterrows():\n    height, width, x_image_position,y_image_position,z_image_position,x_pixel_spacing,y_pixel_spacing = get_shape(row['file_path_1'])\n    coor_level_df.at[index, 'height'] = height\n    coor_level_df.at[index, 'width'] = width\n    coor_level_df.at[index, 'x_image_position'] = x_image_position\n    coor_level_df.at[index, 'y_image_position'] = y_image_position\n    coor_level_df.at[index, 'z_image_position'] = z_image_position\n    coor_level_df.at[index, 'x_pixel_spacing'] = x_pixel_spacing\n    coor_level_df.at[index, 'y_pixel_spacing'] = y_pixel_spacing","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.369908Z","iopub.execute_input":"2024-07-27T19:40:54.370449Z","iopub.status.idle":"2024-07-27T19:40:54.405785Z","shell.execute_reply.started":"2024-07-27T19:40:54.370382Z","shell.execute_reply":"2024-07-27T19:40:54.404369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepares data frame for model\nimage_data_df = coor_level_df[['series_description',\n                           'level',\n                           'height','width',\n                           'x_image_position',\n                           'y_image_position',\n                           'z_image_position',\n                           'x_pixel_spacing',\n                           'y_pixel_spacing',]\n                           ]\n\nlevel_values = {'l1_l2': 1, 'l2_l3': 2, 'l3_l4': 3, 'l4_l5': 4, 'l5_s1': 5}\nseries_values = {'Sagittal T2/STIR': 1, 'Sagittal T1': 2}\n# Replace values with integers\nimage_data_df['series_description'] = image_data_df['series_description'].replace(series_values)\nimage_data_df['level'] = image_data_df['level'].replace(level_values)\nimage_data_df = image_data_df.apply(pd.to_numeric, errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.407286Z","iopub.execute_input":"2024-07-27T19:40:54.407731Z","iopub.status.idle":"2024-07-27T19:40:54.422785Z","shell.execute_reply.started":"2024-07-27T19:40:54.407676Z","shell.execute_reply":"2024-07-27T19:40:54.421495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_data_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.424623Z","iopub.execute_input":"2024-07-27T19:40:54.425101Z","iopub.status.idle":"2024-07-27T19:40:54.450299Z","shell.execute_reply.started":"2024-07-27T19:40:54.425053Z","shell.execute_reply":"2024-07-27T19:40:54.448078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sclaes data and inputs into model\ny_input_df = scaler.transform(image_data_df)\nlevel_coordinates = y_coor_model.predict(y_input_df)\n# Add the coordinates to the dataframe\ncoor_level_df['level_coordinate'] = level_coordinates","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.453222Z","iopub.execute_input":"2024-07-27T19:40:54.453947Z","iopub.status.idle":"2024-07-27T19:40:54.470497Z","shell.execute_reply.started":"2024-07-27T19:40:54.453892Z","shell.execute_reply":"2024-07-27T19:40:54.469064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drops columns not needed calculate Axial image\ncolumns_to_drop = ['series_description',\n                  'width',\n                   'x_image_position',\n                  'z_image_position',\n                  'x_pixel_spacing',\n                  ]\nspace_df =coor_level_df.drop(columns = columns_to_drop)\nspace_df['distance_from_reference'] = space_df['y_image_position'] + ((space_df['height'] - space_df['level_coordinate'])*space_df['y_pixel_spacing']) ","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.472378Z","iopub.execute_input":"2024-07-27T19:40:54.472792Z","iopub.status.idle":"2024-07-27T19:40:54.484336Z","shell.execute_reply.started":"2024-07-27T19:40:54.472761Z","shell.execute_reply":"2024-07-27T19:40:54.482841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.486255Z","iopub.execute_input":"2024-07-27T19:40:54.486694Z","iopub.status.idle":"2024-07-27T19:40:54.516509Z","shell.execute_reply.started":"2024-07-27T19:40:54.486660Z","shell.execute_reply":"2024-07-27T19:40:54.515292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get Axial Instance","metadata":{}},{"cell_type":"code","source":"#function to get Z axis position of axial plane images\ndef get_axial_z_positions(image_paths):\n    positions = {}\n    for path in image_paths:\n        dicom = pydicom.dcmread(path)\n        z_image_position = dicom.ImagePositionPatient[2]  # Z position\n        positions[path] = z_image_position\n    return positions","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.529554Z","iopub.execute_input":"2024-07-27T19:40:54.530143Z","iopub.status.idle":"2024-07-27T19:40:54.543789Z","shell.execute_reply.started":"2024-07-27T19:40:54.530096Z","shell.execute_reply":"2024-07-27T19:40:54.542058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_image_paths(space_df):\n    file_paths_dict = {}\n\n    for index, row in space_df.iterrows():\n        study_id = row['study_id']\n        series_id = row['series_id']\n        y_postion = row['distance_from_reference']\n        \n        # Get file paths and z positions\n        file_paths = get_file_list(study_id, series_id)\n        z_positions = get_axial_z_positions(file_paths)\n        # Calculate the reference z position\n        ref_z = y_postion\n        \n        # Find the closest image based on z-axis position\n        closest_path = None\n        min_distance = float('inf')\n        for path, z in z_positions.items():\n            distance = abs(z - ref_z)\n            if distance < min_distance:\n                min_distance = distance\n                closest_path = path\n        \n        # Add the closest file path to the dataframe\n        space_df.at[index, 'file_path'] = closest_path\n\n    return space_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.545764Z","iopub.execute_input":"2024-07-27T19:40:54.546227Z","iopub.status.idle":"2024-07-27T19:40:54.558307Z","shell.execute_reply.started":"2024-07-27T19:40:54.546159Z","shell.execute_reply":"2024-07-27T19:40:54.556510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axial_temp_df = test_df[(test_df['series_description'] == 'Axial T2')]\nmerged_df = pd.merge(space_df, axial_temp_df, on='study_id', suffixes=('', '_new')) \nmerged_df['series_id'] = merged_df['series_id_new']\nupdated_space_df = merged_df.drop(columns=['series_id_new'])\nupdated_space_df = match_image_paths(updated_space_df)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-27T19:40:54.559900Z","iopub.execute_input":"2024-07-27T19:40:54.560329Z","iopub.status.idle":"2024-07-27T19:40:54.925312Z","shell.execute_reply.started":"2024-07-27T19:40:54.560263Z","shell.execute_reply":"2024-07-27T19:40:54.923378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\n\nupdated_space_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.927326Z","iopub.execute_input":"2024-07-27T19:40:54.927798Z","iopub.status.idle":"2024-07-27T19:40:54.949001Z","shell.execute_reply.started":"2024-07-27T19:40:54.927762Z","shell.execute_reply":"2024-07-27T19:40:54.947319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Axial Data Prep","metadata":{}},{"cell_type":"code","source":"#get list of instance numebrs for each series id folder\ndef get_file_paths_dict(study_id, series_id):\n    data_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n    dir_path = os.path.join(data_dir, str(study_id), str(series_id))\n    files = [file for file in os.listdir(dir_path) if file.endswith('.dcm')]\n    file_paths_dict = {}\n    \n    for file in files:\n        instance_number = int(file.split('.')[0].split('_')[-1])\n        file_paths_dict[instance_number] = os.path.join(dir_path, file)\n    return file_paths_dict","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.950525Z","iopub.execute_input":"2024-07-27T19:40:54.950949Z","iopub.status.idle":"2024-07-27T19:40:54.960499Z","shell.execute_reply.started":"2024-07-27T19:40:54.950917Z","shell.execute_reply":"2024-07-27T19:40:54.959180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#gets the adjacent instance numebr to the predicted one\ndef get_adjacent_file_paths(row, file_paths_dict):\n    instance_number = int(row['instance_number'])\n    file_path_minus_1 = file_paths_dict.get(instance_number - 1, file_paths_dict[instance_number])\n    file_path_current = file_paths_dict.get(instance_number, file_paths_dict[instance_number])\n    file_path_plus_1 = file_paths_dict.get(instance_number + 1, file_paths_dict[instance_number])\n    \n    return pd.Series([file_path_minus_1, file_path_current, file_path_plus_1],\n                     index=['file_path_1', 'file_path_2', 'file_path_3'])","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.961942Z","iopub.execute_input":"2024-07-27T19:40:54.962344Z","iopub.status.idle":"2024-07-27T19:40:54.975224Z","shell.execute_reply.started":"2024-07-27T19:40:54.962301Z","shell.execute_reply":"2024-07-27T19:40:54.973845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to combine all of the file paths of each level\ndef consolidate_file_paths(df, levels):\n    grouped = df.groupby('study_id')\n    results = []\n    \n    for name, group in grouped:\n        # Sort the group based on the level order\n        group['level'] = pd.Categorical(group['level'], categories=levels, ordered=True)\n        group = group.sort_values('level')\n        \n        # Collect the file paths\n        file_paths = group[['file_path_1', 'file_path_2', 'file_path_3']].values.flatten().tolist()\n        \n        # Ensure we have exactly 15 paths, filling with the next available if needed\n        consolidated_paths = []\n        for i in range(15):\n            if i < len(file_paths):\n                consolidated_paths.append(file_paths[i])\n            else:\n                consolidated_paths.append(file_paths[i % len(file_paths)])\n        \n        # Create a new row with the study_id and the consolidated paths\n        result = [name] + consolidated_paths\n        results.append(result)\n    \n    # Create a new DataFrame from the results\n    columns = ['study_id'] + [f'file_path_{i+1}' for i in range(15)]\n    consolidated_df = pd.DataFrame(results, columns=columns)\n    \n    return consolidated_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.976803Z","iopub.execute_input":"2024-07-27T19:40:54.977214Z","iopub.status.idle":"2024-07-27T19:40:54.990092Z","shell.execute_reply.started":"2024-07-27T19:40:54.977179Z","shell.execute_reply":"2024-07-27T19:40:54.988652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_space_df['instance_number'] = updated_space_df['file_path'].str.extract(r'(\\d+)\\.dcm$')\n\nupdated_space_df[['file_path_1', 'file_path_2', 'file_path_3']] = None\n\n# Apply the function to each row\nfor index, row in updated_space_df.iterrows():\n    file_paths_dict = get_file_paths_dict(row['study_id'], row['series_id'])\n    updated_space_df.loc[index, ['file_path_1', 'file_path_2', 'file_path_3']] = get_adjacent_file_paths(row, file_paths_dict)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:54.991819Z","iopub.execute_input":"2024-07-27T19:40:54.992552Z","iopub.status.idle":"2024-07-27T19:40:55.026163Z","shell.execute_reply.started":"2024-07-27T19:40:54.992504Z","shell.execute_reply":"2024-07-27T19:40:55.025092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drops unneed columns for model\nupdated_space_df.groupby(['study_id', 'series_id', 'level']).first().reset_index()\ncolumns_to_drop = ['series_id',\n                   'instance_number',\n                   'file_path',\n                   'height',\n                   'y_image_position',\n                   'y_pixel_spacing',\n                   'distance_from_reference',\n                   'series_description',\n                   'level_coordinate'\n                  ]\nupdated_space_df.drop(columns=columns_to_drop, inplace=True)\naxial_df = consolidate_file_paths(updated_space_df, levels)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:55.027461Z","iopub.execute_input":"2024-07-27T19:40:55.027779Z","iopub.status.idle":"2024-07-27T19:40:55.045665Z","shell.execute_reply.started":"2024-07-27T19:40:55.027751Z","shell.execute_reply":"2024-07-27T19:40:55.044364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axial_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:55.047341Z","iopub.execute_input":"2024-07-27T19:40:55.047823Z","iopub.status.idle":"2024-07-27T19:40:55.076340Z","shell.execute_reply.started":"2024-07-27T19:40:55.047780Z","shell.execute_reply":"2024-07-27T19:40:55.074649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"def predict_single_image(model, image_paths, device, num_labels, num_classes, transform):\n    images = []\n\n    for image_path in image_paths:\n        dicom_data = pydicom.dcmread(image_path)\n        image = dicom_data.pixel_array.astype(float)\n\n        if dicom_data.PhotometricInterpretation == \"MONOCHROME1\":\n            image = np.amax(image) - image\n\n        # Standardize the image\n        image = (image - np.mean(image)) / np.std(image)\n        image = Image.fromarray((image * 255).astype(np.uint8))\n\n        if transform:\n            image = transform(image)\n\n        images.append(image)\n\n    # Concatenate images along the channel dimension\n    input_tensor = torch.cat(images, dim=0).unsqueeze(0)\n\n    input_tensor = input_tensor.to(device)\n    model = model.to(device)\n\n    # Perform inference\n    model.eval()\n    with torch.no_grad():\n        outputs = model(input_tensor)\n\n    # Reshape outputs to batch_size, num_labels, num_classes\n    batch_size = outputs.shape[0]\n    outputs = outputs.view(batch_size, num_labels, num_classes)\n\n    # Apply softmax to get probabilities\n    probabilities = torch.softmax(outputs, dim=2)\n\n    # Convert tensor to list\n    predictions = probabilities.squeeze().tolist()\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:55.078090Z","iopub.execute_input":"2024-07-27T19:40:55.078629Z","iopub.status.idle":"2024-07-27T19:40:55.089836Z","shell.execute_reply.started":"2024-07-27T19:40:55.078595Z","shell.execute_reply":"2024-07-27T19:40:55.088451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictions(df, model, device, plane):\n    results = []\n    \n    # Define the transformation used during training\n    transform = transforms.Compose([\n        transforms.Resize((CFG.image_size[0], CFG.image_size[1])),\n        transforms.ToTensor(),\n        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n    ])\n\n    # Define the levels\n    levels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']\n    \n    # Determine number of labels based on the model and series_description\n    if plane == 'sagittal':\n        num_labels = CFG.sag_labels\n        num_images = CFG.sag_channels\n        conditions = ['spinal_canal_stenosis', 'left_neural_foraminal_narrowing', 'right_neural_foraminal_narrowing']\n    else:\n        num_labels = CFG.axial_labels\n        num_images = CFG.axial_channels\n        conditions = ['left_subarticular_stenosis', 'right_subarticular_stenosis']\n    \n    for _, row in df.iterrows():\n        study_id = row['study_id']\n        \n        # Collects all image paths based on the number of images expected\n        image_paths = [row[f'file_path_{i+1}'] for i in range(num_images)]\n        \n        # Performs predictions\n        preds = predict_single_image(model, image_paths, device, num_labels, CFG.num_classes, transform)\n        \n        #breaks up the predictions by condtion and level and puts them in the right order\n        for i, condition in enumerate(conditions):\n            for j, level in enumerate(levels):\n                row_id = f\"{study_id}_{condition}_{level}\"\n                normal_mild = preds[i * len(levels) + j][0]\n                moderate = preds[i * len(levels) + j][1]\n                severe = preds[i * len(levels) + j][2]\n                \n                results.append([row_id, normal_mild, moderate, severe])\n    \n    # Create a DataFrame for the results\n    results_df = pd.DataFrame(results, columns=['row_id', 'normal_mild', 'moderate', 'severe'])\n    \n    return results_df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:55.091707Z","iopub.execute_input":"2024-07-27T19:40:55.092634Z","iopub.status.idle":"2024-07-27T19:40:55.107571Z","shell.execute_reply.started":"2024-07-27T19:40:55.092582Z","shell.execute_reply":"2024-07-27T19:40:55.106028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plane = 'sagittal'\nsag_results_df = predictions(sagittal_df, sag_model, device, plane)\nplane = 'axial'\naxial_results_df = predictions(axial_df, axial_model, device, plane)\npredictions_df = pd.concat([sag_results_df, axial_results_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:55.109092Z","iopub.execute_input":"2024-07-27T19:40:55.109502Z","iopub.status.idle":"2024-07-27T19:40:58.136137Z","shell.execute_reply.started":"2024-07-27T19:40:55.109471Z","shell.execute_reply":"2024-07-27T19:40:58.134807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_normalize = ['normal_mild', 'moderate', 'severe']\npredictions_df[cols_to_normalize] = predictions_df[cols_to_normalize].div(predictions_df[cols_to_normalize].sum(axis=1), axis=0)\npredictions_df = predictions_df.sort_values(by='row_id')\npredictions_df","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:58.137996Z","iopub.execute_input":"2024-07-27T19:40:58.138460Z","iopub.status.idle":"2024-07-27T19:40:58.169515Z","shell.execute_reply.started":"2024-07-27T19:40:58.138421Z","shell.execute_reply":"2024-07-27T19:40:58.167989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df.to_csv('submission.csv', float_format='%.10f', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:40:58.171414Z","iopub.execute_input":"2024-07-27T19:40:58.172442Z","iopub.status.idle":"2024-07-27T19:40:58.182108Z","shell.execute_reply.started":"2024-07-27T19:40:58.172387Z","shell.execute_reply":"2024-07-27T19:40:58.180385Z"},"trusted":true},"execution_count":null,"outputs":[]}]}