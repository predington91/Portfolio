{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os, gc, sys, copy, pickle\nfrom pathlib import Path\nimport glob\nimport joblib\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T23:19:01.951094Z","iopub.execute_input":"2024-07-23T23:19:01.951494Z","iopub.status.idle":"2024-07-23T23:19:03.042975Z","shell.execute_reply.started":"2024-07-23T23:19:01.951463Z","shell.execute_reply":"2024-07-23T23:19:03.041765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial Cleaning","metadata":{}},{"cell_type":"code","source":"#Imports CSV files needed for notebook\npath = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\n\ntrain_df = pd.read_csv(path + 'train.csv')\ntrain_coordinates_df = pd.read_csv(path + 'train_label_coordinates.csv')\ntrain_desc_df = pd.read_csv(path + 'train_series_descriptions.csv')\ntest_desc_df = pd.read_csv(path + 'test_series_descriptions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.044896Z","iopub.execute_input":"2024-07-23T23:19:03.045529Z","iopub.status.idle":"2024-07-23T23:19:03.223960Z","shell.execute_reply.started":"2024-07-23T23:19:03.045489Z","shell.execute_reply":"2024-07-23T23:19:03.222789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prints size/shapes of all of the dateframes\ndf_names = [\"train_df\", \n            \"train_coordinates_df\", \n            \"train_desc_df\", \n            \"test_desc_df\"\n           ]\n\nfor name in df_names:\n    df = globals()[name]\n    print(f\"{name}: {df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.225339Z","iopub.execute_input":"2024-07-23T23:19:03.225847Z","iopub.status.idle":"2024-07-23T23:19:03.232257Z","shell.execute_reply.started":"2024-07-23T23:19:03.225806Z","shell.execute_reply":"2024-07-23T23:19:03.230840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rearanges the train CSV into 3 columns to be more useful for training\nmelted_df = train_df.melt(id_vars='study_id', var_name='condition', value_name='severity')\n#Spilts the conditions and level classifications\nmelted_df['level'] = melted_df['condition'].str[-5:]\nmelted_df['condition'] = melted_df['condition'].str[:-6]\nmelted_df['level'] = melted_df['level'].str.replace(\"_\",\"/\")\nmelted_df['condition'] = melted_df['condition'].str.replace(\"_\",\" \")\nmelted_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.234752Z","iopub.execute_input":"2024-07-23T23:19:03.235140Z","iopub.status.idle":"2024-07-23T23:19:03.346431Z","shell.execute_reply.started":"2024-07-23T23:19:03.235102Z","shell.execute_reply":"2024-07-23T23:19:03.345317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#joins the melted dataset with the coordinate dataset\ntrain_coordinates_df['condition'] = train_coordinates_df['condition'].str.lower()\ntrain_coordinates_df['level'] = train_coordinates_df['level'].str.lower()\ntraining_df = pd.merge(train_coordinates_df,melted_df, on = ['study_id','condition','level'])\n\n#merges new dataframe with description dataset\ntraining_df = pd.merge(training_df,train_desc_df, on = ['study_id','series_id'])\n\ntraining_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.347488Z","iopub.execute_input":"2024-07-23T23:19:03.347832Z","iopub.status.idle":"2024-07-23T23:19:03.466704Z","shell.execute_reply.started":"2024-07-23T23:19:03.347804Z","shell.execute_reply":"2024-07-23T23:19:03.465586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Spilt the data frame by plane\nsagittal_df = training_df[training_df['series_description'].isin(['Sagittal T2/STIR', 'Sagittal T1'])]\naxial_df = training_df[training_df['series_description'] == 'Axial T2']\nprint(sagittal_df.shape)\nprint(axial_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.468506Z","iopub.execute_input":"2024-07-23T23:19:03.468954Z","iopub.status.idle":"2024-07-23T23:19:03.490528Z","shell.execute_reply.started":"2024-07-23T23:19:03.468915Z","shell.execute_reply":"2024-07-23T23:19:03.489223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prep Sagittal Data","metadata":{}},{"cell_type":"code","source":"#Creates function to get list of fiels in each series folder\ndef get_file_list(study_id, series_id):\n    data_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n    files = os.listdir(os.path.join(data_dir, str(study_id), str(series_id)))\n    file_paths = [os.path.join(str(study_id), str(series_id), file) for file in files if file.endswith('.dcm')]\n    return file_paths","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.492048Z","iopub.execute_input":"2024-07-23T23:19:03.492474Z","iopub.status.idle":"2024-07-23T23:19:03.499749Z","shell.execute_reply.started":"2024-07-23T23:19:03.492435Z","shell.execute_reply":"2024-07-23T23:19:03.498508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expand_dataframe(df, max_files=12):\n    # Prepare a list to store the new columns\n    new_columns = {f'file_path_{i+1}': [] for i in range(max_files)}\n\n    # Iterate through each row in the DataFrame\n    for index, row in df.iterrows():\n        study_id = row['study_id']\n        series_id = row['series_id']\n\n        # Get list of files for the given study_id and series_id\n        file_list = get_file_list(study_id, series_id)\n\n        # If there are fewer files than `max_files`, duplicate the list\n        if len(file_list) < max_files:\n            file_list = (file_list * ((max_files // len(file_list)) + 1))[:max_files]\n        else:\n            file_list = file_list[:max_files]\n\n        # Fill the new columns with file paths\n        for i in range(max_files):\n            new_columns[f'file_path_{i+1}'].append(file_list[i] if i < len(file_list) else None)\n\n    # Convert the new columns into a DataFrame\n    new_columns_df = pd.DataFrame(new_columns)\n    expanded_df = pd.concat([df.reset_index(drop=True), new_columns_df], axis=1)\n\n    return expanded_df","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.501158Z","iopub.execute_input":"2024-07-23T23:19:03.501511Z","iopub.status.idle":"2024-07-23T23:19:03.589365Z","shell.execute_reply.started":"2024-07-23T23:19:03.501483Z","shell.execute_reply":"2024-07-23T23:19:03.588019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_file_paths(df, max_files=12):\n    combined_data = {}\n\n    # Iterate through each row in the DataFrame\n    for _, row in df.iterrows():\n        study_id = row['study_id']\n        \n        # Extract file paths from the row, excluding NaN values\n        file_paths = [row[f'file_path_{i+1}'] for i in range(max_files) if pd.notna(row[f'file_path_{i+1}'])]\n\n        # Initialize the study_id entry in combined_data if not present\n        if study_id not in combined_data:\n            combined_data[study_id] = [None] * (max_files * 2)  # Initialize with None values\n\n        # Find the next available index for the file paths\n        existing_paths = [path for path in combined_data[study_id] if path is not None]\n        start_index = len(existing_paths)\n\n        # Add file paths to the combined data\n        for i, path in enumerate(file_paths):\n            if start_index + i < len(combined_data[study_id]):\n                combined_data[study_id][start_index + i] = path\n\n    # Create a DataFrame from the combined data\n    combined_df = pd.DataFrame.from_dict(combined_data, orient='index', columns=[f'file_path_{i+1}' for i in range(max_files * 2)])\n\n    # Reset index to have study_id as a column\n    combined_df.reset_index(inplace=True)\n    combined_df.rename(columns={'index': 'study_id'}, inplace=True)\n\n    return combined_df","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.590649Z","iopub.execute_input":"2024-07-23T23:19:03.591002Z","iopub.status.idle":"2024-07-23T23:19:03.604220Z","shell.execute_reply.started":"2024-07-23T23:19:03.590972Z","shell.execute_reply":"2024-07-23T23:19:03.602976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to reuse files in order if not avaiabel to fill every column\ndef fill_empty_paths(df):\n    for index, row in df.iterrows():\n        replace_image = row['file_path_1']\n        count = 0\n        for i in range(1, 25):\n            col_name = f'file_path_{i}'\n            if pd.isna(row[col_name]):\n                count += 1\n                replace_image = row[f'file_path_{count}']\n                df.at[index, col_name] = replace_image\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.607551Z","iopub.execute_input":"2024-07-23T23:19:03.607927Z","iopub.status.idle":"2024-07-23T23:19:03.622025Z","shell.execute_reply.started":"2024-07-23T23:19:03.607897Z","shell.execute_reply":"2024-07-23T23:19:03.620824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pulls every image in each folder for every study/series ID\ndrop_df = sagittal_df.drop_duplicates(subset=['study_id', 'series_id'])\nexpanded_df = expand_dataframe(drop_df)\ncombine_df = combine_file_paths(expanded_df)\nsagittal_df = fill_empty_paths(combine_df)\nsagittal_df","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:03.623605Z","iopub.execute_input":"2024-07-23T23:19:03.624016Z","iopub.status.idle":"2024-07-23T23:19:26.580084Z","shell.execute_reply.started":"2024-07-23T23:19:03.623985Z","shell.execute_reply":"2024-07-23T23:19:26.578721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sagittal_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:26.581990Z","iopub.execute_input":"2024-07-23T23:19:26.582465Z","iopub.status.idle":"2024-07-23T23:19:26.597542Z","shell.execute_reply.started":"2024-07-23T23:19:26.582426Z","shell.execute_reply":"2024-07-23T23:19:26.596067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replaces labels with catgorical variables\nlabel2id = {'Normal/Mild': 0, \n            'Moderate':1, \n            'Severe':2\n           }\ntrain_df = train_df.replace(label2id)\n#merges the dataset with the dependent variables\nsag_merge_df = pd.merge(sagittal_df,train_df, on = ['study_id'])\nsag_merge_df.fillna(0,inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:26.599484Z","iopub.execute_input":"2024-07-23T23:19:26.599851Z","iopub.status.idle":"2024-07-23T23:19:26.642340Z","shell.execute_reply.started":"2024-07-23T23:19:26.599821Z","shell.execute_reply":"2024-07-23T23:19:26.641197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sag_merge_df","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:26.643964Z","iopub.execute_input":"2024-07-23T23:19:26.644325Z","iopub.status.idle":"2024-07-23T23:19:26.682511Z","shell.execute_reply.started":"2024-07-23T23:19:26.644295Z","shell.execute_reply":"2024-07-23T23:19:26.681051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prep Axial Data","metadata":{}},{"cell_type":"code","source":"#gets the instance of of every file for each series folder\ndef get_file_paths_dict(study_id, series_id):\n    data_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n    dir_path = os.path.join(data_dir, str(study_id), str(series_id))\n    files = [file for file in os.listdir(dir_path) if file.endswith('.dcm')]\n    file_paths_dict = {}\n    \n    for file in files:\n        instance_number = int(file.split('.')[0].split('_')[-1])\n        file_paths_dict[instance_number] = os.path.join(dir_path, file)\n    return file_paths_dict","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:26.684062Z","iopub.execute_input":"2024-07-23T23:19:26.684531Z","iopub.status.idle":"2024-07-23T23:19:26.692406Z","shell.execute_reply.started":"2024-07-23T23:19:26.684493Z","shell.execute_reply":"2024-07-23T23:19:26.691209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creates fiel paths for the instance numeber and the adjancent files\ndef get_adjacent_file_paths(row, file_paths_dict):\n    instance_number = row['instance_number']\n    file_path_minus_1 = file_paths_dict.get(instance_number - 1, file_paths_dict[instance_number])\n    file_path_current = file_paths_dict.get(instance_number, file_paths_dict[instance_number])\n    file_path_plus_1 = file_paths_dict.get(instance_number + 1, file_paths_dict[instance_number])\n    \n    return pd.Series([file_path_minus_1, file_path_current, file_path_plus_1],\n                     index=['file_path_1', 'file_path_2', 'file_path_3'])","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:26.693728Z","iopub.execute_input":"2024-07-23T23:19:26.694374Z","iopub.status.idle":"2024-07-23T23:19:26.705026Z","shell.execute_reply.started":"2024-07-23T23:19:26.694331Z","shell.execute_reply":"2024-07-23T23:19:26.703817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def consolidate_file_paths(df, levels):\n    grouped = df.groupby('study_id')\n    results = []\n    \n    for name, group in grouped:\n        # Sort the group based on the level order\n        group['level'] = pd.Categorical(group['level'], categories=levels, ordered=True)\n        group = group.sort_values('level')\n        \n        # Collect the file paths\n        file_paths = group[['file_path_1', 'file_path_2', 'file_path_3']].values.flatten().tolist()\n        \n        # verifies tehre are 15 file paths, if not uses the next avaiable\n        consolidated_paths = []\n        for i in range(15):\n            if i < len(file_paths):\n                consolidated_paths.append(file_paths[i])\n            else:\n                consolidated_paths.append(file_paths[i % len(file_paths)])\n        \n        # Creates a new row with the study_id and the consolidated paths\n        result = [name] + consolidated_paths\n        results.append(result)\n    \n    # Creates a new DataFrame from the results\n    columns = ['study_id'] + [f'file_path_{i+1}' for i in range(15)]\n    consolidated_df = pd.DataFrame(results, columns=columns)\n    \n    return consolidated_df","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:26.706654Z","iopub.execute_input":"2024-07-23T23:19:26.707770Z","iopub.status.idle":"2024-07-23T23:19:26.719624Z","shell.execute_reply.started":"2024-07-23T23:19:26.707708Z","shell.execute_reply":"2024-07-23T23:19:26.718288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axial_df[['file_path_1', 'file_path_2', 'file_path_3']] = None\n\n# Apply the function to each row\nfor index, row in axial_df.iterrows():\n    file_paths_dict = get_file_paths_dict(row['study_id'], row['series_id'])\n    axial_df.loc[index, ['file_path_1', 'file_path_2', 'file_path_3']] = get_adjacent_file_paths(row, file_paths_dict)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:19:26.721339Z","iopub.execute_input":"2024-07-23T23:19:26.721700Z","iopub.status.idle":"2024-07-23T23:20:37.258032Z","shell.execute_reply.started":"2024-07-23T23:19:26.721671Z","shell.execute_reply":"2024-07-23T23:20:37.256810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axial_df","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:20:37.259438Z","iopub.execute_input":"2024-07-23T23:20:37.259808Z","iopub.status.idle":"2024-07-23T23:20:37.279996Z","shell.execute_reply.started":"2024-07-23T23:20:37.259777Z","shell.execute_reply":"2024-07-23T23:20:37.278578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#groups by study/series IDs and level to get rid of duplicates found in data\naxial_df.groupby(['study_id', 'series_id', 'level']).first().reset_index()\n#drops columns no longer needed\ncolumns_to_drop = ['series_id',\n                   'instance_number',\n                   'condition',\n                   'x',\n                   'y',\n                   'severity',\n                   'series_description'\n                  ]\naxial_df.drop(columns=columns_to_drop, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:32:42.486768Z","iopub.execute_input":"2024-07-23T23:32:42.487166Z","iopub.status.idle":"2024-07-23T23:32:42.534468Z","shell.execute_reply.started":"2024-07-23T23:32:42.487136Z","shell.execute_reply":"2024-07-23T23:32:42.533359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creates a list of the different levels\nlevels = [\n    'l1_l2',\n    'l2_l3',\n    'l3_l4',\n    'l4_l5',\n    'l5_s1',\n]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:32:59.784955Z","iopub.execute_input":"2024-07-23T23:32:59.785326Z","iopub.status.idle":"2024-07-23T23:33:02.275542Z","shell.execute_reply.started":"2024-07-23T23:32:59.785299Z","shell.execute_reply":"2024-07-23T23:33:02.274396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combines all of the file paths for each study and series ID\naxial_combine_df = consolidate_file_paths(axial_df, levels)\naxial_combine_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:58:58.239765Z","iopub.execute_input":"2024-08-04T16:58:58.240721Z","iopub.status.idle":"2024-08-04T16:58:58.707246Z","shell.execute_reply.started":"2024-08-04T16:58:58.240669Z","shell.execute_reply":"2024-08-04T16:58:58.704908Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m axial_combine_df \u001b[38;5;241m=\u001b[39m \u001b[43mconsolidate_file_paths\u001b[49m(axial_df, levels)\n\u001b[1;32m      2\u001b[0m axial_combine_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'consolidate_file_paths' is not defined"],"ename":"NameError","evalue":"name 'consolidate_file_paths' is not defined","output_type":"error"}]},{"cell_type":"code","source":"#Merges file path data frame with training dataframe\naxial_merge_df = pd.merge(axial_combine_df,train_df, on = ['study_id'])\naxial_merge_df.fillna(0,inplace=True)\n\naxial_merge_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:33:49.991250Z","iopub.execute_input":"2024-07-23T23:33:49.991714Z","iopub.status.idle":"2024-07-23T23:33:50.011300Z","shell.execute_reply.started":"2024-07-23T23:33:49.991677Z","shell.execute_reply":"2024-07-23T23:33:50.009978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FInalize Both Data Frames","metadata":{}},{"cell_type":"code","source":"#Creates lists of each condition type for the specific MRI image description\nsagittal_conditions_drop = [\n    'left_subarticular_stenosis',\n    'right_subarticular_stenosis'\n]\n\naxial_conditions_drop = [\n    'spinal_canal_stenosis', \n    'left_neural_foraminal_narrowing', \n    'right_neural_foraminal_narrowing',\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:34:23.786306Z","iopub.execute_input":"2024-07-23T23:34:23.786698Z","iopub.status.idle":"2024-07-23T23:34:23.792057Z","shell.execute_reply.started":"2024-07-23T23:34:23.786668Z","shell.execute_reply":"2024-07-23T23:34:23.790927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removes the functions that aren't used in each plane\ndef remove_coditions(df, conditions, levels):\n    df_copy = df.copy()\n    column_drop = []\n    for c in conditions:\n        for l in levels:\n            column_drop.append(c + '_' + l)\n            \n    df_copy.drop(columns=column_drop,inplace=True)\n    \n    return df_copy","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:34:25.250556Z","iopub.execute_input":"2024-07-23T23:34:25.250990Z","iopub.status.idle":"2024-07-23T23:34:25.257662Z","shell.execute_reply.started":"2024-07-23T23:34:25.250959Z","shell.execute_reply":"2024-07-23T23:34:25.256449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#runs th eprevious function\nsagittal_cleaned = remove_coditions(sag_merge_df, sagittal_conditions_drop, levels)\naxial_cleaned = remove_coditions(axial_merge_df, axial_conditions_drop, levels)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:34:26.955704Z","iopub.execute_input":"2024-07-23T23:34:26.956909Z","iopub.status.idle":"2024-07-23T23:34:26.969139Z","shell.execute_reply.started":"2024-07-23T23:34:26.956870Z","shell.execute_reply":"2024-07-23T23:34:26.967947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sagittal_cleaned.shape)\nprint(axial_cleaned.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:34:28.674622Z","iopub.execute_input":"2024-07-23T23:34:28.675409Z","iopub.status.idle":"2024-07-23T23:34:28.680655Z","shell.execute_reply.started":"2024-07-23T23:34:28.675367Z","shell.execute_reply":"2024-07-23T23:34:28.679527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#One hot encodes the dataframes\ndef one_hot_encode(df, num_encode):\n    columns_to_encode = df.columns[-num_encode:]\n    return pd.get_dummies(df, columns=columns_to_encode)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:34:35.104834Z","iopub.execute_input":"2024-07-23T23:34:35.105225Z","iopub.status.idle":"2024-07-23T23:34:35.111052Z","shell.execute_reply.started":"2024-07-23T23:34:35.105193Z","shell.execute_reply":"2024-07-23T23:34:35.109714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sagittal_final = one_hot_encode(sagittal_cleaned,15)\naxial_final = one_hot_encode(axial_cleaned,10)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:34:36.963591Z","iopub.execute_input":"2024-07-23T23:34:36.964092Z","iopub.status.idle":"2024-07-23T23:34:37.008095Z","shell.execute_reply.started":"2024-07-23T23:34:36.964052Z","shell.execute_reply":"2024-07-23T23:34:37.006877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sagittal_final.shape)\nprint(axial_final.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:34:38.754222Z","iopub.execute_input":"2024-07-23T23:34:38.754587Z","iopub.status.idle":"2024-07-23T23:34:38.760542Z","shell.execute_reply.started":"2024-07-23T23:34:38.754560Z","shell.execute_reply":"2024-07-23T23:34:38.759388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sagittal_final.to_csv('sagittal__df.csv', index=False)\naxial_final.to_csv('axial_df.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:34:41.165309Z","iopub.execute_input":"2024-07-23T23:34:41.165686Z","iopub.status.idle":"2024-07-23T23:34:41.368921Z","shell.execute_reply.started":"2024-07-23T23:34:41.165658Z","shell.execute_reply":"2024-07-23T23:34:41.367531Z"},"trusted":true},"execution_count":null,"outputs":[]}]}